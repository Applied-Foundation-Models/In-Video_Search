{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "from src.video_preprocessing.download_videos.youtube_download import preprocess_video\n",
    "from src.video_preprocessing.scene_detection.scene_detect import detect_scenes\n",
    "from src.video_preprocessing.download_videos.download_utils import (\n",
    "    transcribe_audio_files,\n",
    "    extract_and_store_audio,\n",
    ")\n",
    "from src.ocr.pytesseract_image_to_text import extract_text_from_image\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from src.text_embedder.embedder import text_to_embedding_transformer\n",
    "\n",
    "from src.llm.ollama_implementation.ollama_experiment import (\n",
    "    prompt_llm_summary,\n",
    "    generate_caption_using_llava,\n",
    "    prompt_llm_extensive_summary,\n",
    ")\n",
    "from src.video_preprocessing.download_videos.download_utils import (\n",
    "    transcription_to_text,\n",
    "    create_metadata,\n",
    ")\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "from loguru import logger\n",
    "import pickle\n",
    "\n",
    "from src.clip.clip_model import CLIPEmbeddingsModel\n",
    "\n",
    "import tqdm\n",
    "from pathlib import Path\n",
    "import os"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "ed097945bf783f0"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Load Data",
   "id": "18eb4cd1dfbcf75c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Load csv file\n",
    "filename = \"test_frames_gt.csv\"\n",
    "df = pd.read_csv(filename)\n",
    "df"
   ],
   "id": "4a36f9c86e3e18ac"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# print pickle file \n",
    "with open(\"bio_3_3_th5.pickle\", \"rb\") as file:\n",
    "    data = pickle.load(file)\n",
    "\n",
    "logger.info(f\"new_pickle: {data}\")"
   ],
   "id": "a7e3891b6d32bf22"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# set image paths for the experiments\n",
    "\n",
    "clip_model = CLIPEmbeddingsModel()\n",
    "\n",
    "extracted_data_path = [data[key]['img_path'] for key in data.keys() if\n",
    "                       'img_path' in data[key]]\n",
    "\n",
    "clip_model.img_paths = None\n",
    "\n",
    "clip_model.img_paths = extracted_data_path\n",
    "\n",
    "extracted_data_ocr_text = [data[key]['ocr_extracted_text'] for key in data.keys() if\n",
    "                           'ocr_extracted_text' in data[key]]\n",
    "\n",
    "logger.info(f\"Extracted_data_ocr_text: {extracted_data_ocr_text}\")\n",
    "\n",
    "logger.info(f\"extracted_data_path: {extracted_data_path}\")\n",
    "\n",
    "# Note: the data is correctly mapped \n",
    "print(extracted_data_path[0])\n",
    "print(extracted_data_ocr_text[0])"
   ],
   "id": "7eed7ee4744bd9d0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# create dataframe to save results of the experiments\n",
    "columns = ['Prompt', 'GT_Keyframe', 'Top_1', 'Top_2', 'Top_3']\n",
    "\n",
    "df_test = pd.DataFrame(columns=columns)\n",
    "\n",
    "df_ocr_only = df_test\n",
    "df_ocr_lava = df_test\n",
    "df_ocr_transcriptions = df_test\n",
    "\n",
    "df_short_llm_summary = df_test\n",
    "df_extensive_summary = df_test\n",
    "\n",
    "df_clip_llm_summary = df_test\n",
    "df_clip_extensive_summary = df_test"
   ],
   "id": "64578e40fcd3a6ed"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def add_to_df(prompt, gt_keyframe, result):\n",
    "    # Create a new row with the provided data\n",
    "    return {\n",
    "        'Prompt': prompt,\n",
    "        'GT_Keyframe': gt_keyframe,\n",
    "        'Top_1': extract_keyframe_number(result[0]) if len(result) > 0 else None,\n",
    "        'Top_2': extract_keyframe_number(result[1]) if len(result) > 1 else None,\n",
    "        'Top_3': extract_keyframe_number(result[2]) if len(result) > 2 else None\n",
    "    }\n",
    "\n",
    "# ietrate over the dataframe and get the results\n",
    "def get_results(df):\n",
    "    for _, row in df.iterrows():\n",
    "        logger.info(row['Prompt'])\n",
    "        prompt = row['Prompt']\n",
    "        gt_keyframe = row['GT_Keyframe']\n",
    "\n",
    "        # Search for similar images\n",
    "        output = clip_model.search_similar_images_top_3(prompt, gt_keyframe)\n",
    "        res_row = add_to_df(prompt, gt_keyframe, output)\n",
    "        rows.append(res_row)\n",
    "\n",
    "    return rows\n",
    "\n",
    "import os\n",
    "\n",
    "def extract_keyframe_number(path):\n",
    "    \"\"\"\n",
    "    Extracts the scene number from the given file path.\n",
    "\n",
    "    Parameters:\n",
    "    path (str): The full path of the file.\n",
    "\n",
    "    Returns:\n",
    "    str: The extracted scene number.\n",
    "    \"\"\"\n",
    "    # Get filename without extension\n",
    "    filename = os.path.splitext(os.path.basename(path))[0]\n",
    "    \n",
    "    # Extract '032' from filename\n",
    "    scene_number = filename.split('-Scene-')[-1].split('-')[0]\n",
    "    \n",
    "    return scene_number"
   ],
   "id": "b9b4e99d4b8ea54"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Embedded with standard Tokenizer: Only OCR\n",
    "clip_model.text_embeddings = None\n",
    "\n",
    "logger.info(f\"Embedded with standard Tokenizer: Only OCR\")\n",
    "\n",
    "extracted_data_ocr_text = [data[key]['ocr_extracted_text'] for key in data.keys() if\n",
    "                           'ocr_extracted_text' in data[key]]\n",
    "\n",
    "#logger.info(extracted_data_ocr_text)\n",
    "\n",
    "# get the embedder model\n",
    "embedder_model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')\n",
    "\n",
    "ocr_embeddings = text_to_embedding_transformer(extracted_data_ocr_text, embedder_model)\n",
    "\n",
    "#logger.info('OCR Embeddings: ', ocr_embeddings)\n",
    "\n",
    "clip_model.text_embeddings = ocr_embeddings\n",
    "\n",
    "rows = get_results(df)\n",
    "\n",
    "df_ocr_only = pd.DataFrame(rows, columns=['Prompt', 'GT_Keyframe', 'Top_1', 'Top_2', 'Top_3'])\n",
    "\n",
    "# Save dataframe to CSV\n",
    "df_ocr_only.to_csv('df_ocr_only.csv', index=False)"
   ],
   "id": "3e9c5eb48242c71c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "bd96b30ced9483bf"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# LOAD MODEL ",
   "id": "b9f59e530248b7eb"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Embedded with standard Tokenizer: OCR * Transcriptions \n",
    "logger.info(f\"Embedded with standard Tokenizer: OCR * Transcriptions\")\n",
    "\n",
    "clip_model.text_embeddings = None\n",
    "\n",
    "extracted_data_ocr_text = [data[key]['ocr_extracted_text'] for key in data.keys() if\n",
    "                           'ocr_extracted_text' in data[key]]\n",
    "\n",
    "extracted_data_transcriptions = [data[key]['transcription'] for key in data.keys() if\n",
    "                                 'transcription' in data[key]]\n",
    "\n",
    "# concatenate ocr and transcriptions\n",
    "concat_result = [a + ' ' + b for a, b in zip(extracted_data_ocr_text, extracted_data_transcriptions)]\n",
    "\n",
    "# get the embedder model\n",
    "embedder_model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')\n",
    "\n",
    "concat_result_embeddings = text_to_embedding_transformer(concat_result, embedder_model)\n",
    "\n",
    "clip_model.text_embeddings = concat_result_embeddings\n",
    "\n",
    "result = []\n",
    "\n",
    "rows = get_results(df)\n",
    "\n",
    "df_ocr_transcriptions = pd.DataFrame(rows, columns=['Prompt', 'GT_Keyframe', 'Top_1', 'Top_2', 'Top_3'])\n",
    "\n",
    "# save on disk\n",
    "df_ocr_transcriptions.to_csv('df_ocr_transcriptions.csv', index=False)"
   ],
   "id": "d252f3a35e617b3a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Embedded with standard Tokenizer: OCR * LLAVA\n",
    "logger.info(f\"Embedded with standard Tokenizer: OCR * LLAVA\")\n",
    "\n",
    "clip_model.text_embeddings = None\n",
    "\n",
    "clip_model.img_paths = None\n",
    "\n",
    "clip_model.img_paths = extracted_data_path\n",
    "\n",
    "extracted_data_ocr_text = [data[key]['ocr_extracted_text'] for key in data.keys() if\n",
    "                           'ocr_extracted_text' in data[key]]\n",
    "\n",
    "extracted_data_llava_result = [data[key]['llava_result'] for key in data.keys() if\n",
    "                               'llava_result' in data[key]]\n",
    "\n",
    "# concatenate ocr and transcriptions\n",
    "concat_result = [a + ' ' + b for a, b in zip(extracted_data_ocr_text, extracted_data_llava_result)]\n",
    "\n",
    "# get the embedder model\n",
    "embedder_model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')\n",
    "\n",
    "concat_result_embeddings = text_to_embedding_transformer(concat_result, embedder_model)\n",
    "\n",
    "clip_model.text_embeddings = concat_result_embeddings\n",
    "\n",
    "result = []\n",
    "\n",
    "rows = get_results(df)\n",
    "\n",
    "df_ocr_lava = pd.DataFrame(rows, columns=['Prompt', 'GT_Keyframe', 'Top_1', 'Top_2', 'Top_3'])\n",
    "\n",
    "# save on disk\n",
    "df_ocr_lava.to_csv('df_ocr_lava.csv', index=False)"
   ],
   "id": "3f420fe808cb9bb9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Embedded with standard Tokenizer - short_llm_summary: OCR * Transcriptions * LLAVA \n",
    "# TODO: Need to get embedding with standard tokenizer for short_llm_summary\n",
    "\n",
    "\n",
    "logger.info(f\"Embedded with standard Tokenizer - clip_llm_summary: OCR * LLAVA * Transcriptions\")\n",
    "\n",
    "clip_model.text_embeddings = None\n",
    "\n",
    "# Assuming that the pickle file has standard tokenizer embeddings \n",
    "extracted_data_text = [data[key]['ocr_extracted_text'] for key in data.keys() if\n",
    "                       'ocr_extracted_text' in data[key]]\n",
    "\n",
    "extracted_data_transcriptions = [data[key]['transcription'] for key in data.keys() if\n",
    "                                 'transcription' in data[key]]\n",
    "\n",
    "extracted_data_llava_result = [data[key]['llava_result'] for key in data.keys() if\n",
    "                               'llava_result' in data[key]]\n",
    "\n",
    "\n"
   ],
   "id": "b79d18c25e4a3dfc"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# print pickle file \n",
    "with open(\"data_standard_tokenizer.pickle\", \"rb\") as file:\n",
    "    data_std_tokenizer = pickle.load(file)\n",
    "\n",
    "logger.info(f\"new_pickle: {data_std_tokenizer}\")"
   ],
   "id": "28c9af65552c2898"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Embedded with standard Tokenizer - extensive_summary: OCR * Transcriptions * LLAVA\n",
    "# TODO: Need to get embedding with standard tokenizer for short_llm_summary\n",
    "\n",
    "logger.info(f\"Embedded with standard Tokenizer - extensive_summary : OCR * LLAVA * Transcriptions\")\n",
    "\n",
    "clip_model.text_embeddings = None\n",
    "\n",
    "# Assuming that the pickle file has standard tokenizer embeddings \n",
    "extracted_data_text = [data[key]['ocr_extracted_text'] for key in data.keys() if\n",
    "                       'ocr_extracted_text' in data[key]]\n",
    "\n",
    "extracted_data_transcriptions = [data[key]['transcription'] for key in data.keys() if\n",
    "                                 'transcription' in data[key]]\n",
    "\n",
    "extracted_data_llava_result = [data[key]['llava_result'] for key in data.keys() if\n",
    "                               'llava_result' in data[key]]\n"
   ],
   "id": "5e1cca4ab98b38dd"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Embedded with CLIP - clip_llm_summary: OCR * Transcriptions * LLAVA \n",
    "logger.info(f\" Embedded with CLIP - clip_llm_summary: OCR * Transcriptions * LLAVA\")\n",
    "\n",
    "clip_model.text_embeddings = None\n",
    "\n",
    "clip_model.img_paths = None\n",
    "\n",
    "clip_model.img_paths = extracted_data_path\n",
    "\n",
    "extracted_data_text = [data[key]['clip_text_embedding'] for key in data.keys() if\n",
    "                       'clip_text_embedding' in data[key]]\n",
    "\n",
    "clip_text_embeddings = [data[0] for data in extracted_data_text]\n",
    "\n",
    "clip_model.text_embeddings = clip_text_embeddings\n",
    "\n",
    "if isinstance(clip_model.text_embeddings, list):\n",
    "    for i, text_embedding in enumerate(clip_model.text_embeddings):\n",
    "        clip_model.text_embeddings[i] = torch.tensor(text_embedding)\n",
    "\n",
    "# create one single torch for sim search \n",
    "clip_model.text_embeddings = torch.stack(clip_model.text_embeddings, dim=0)\n",
    "\n",
    "for i, row in df.iterrows():\n",
    "    logger.info(row['Prompt'])\n",
    "    # get prompt \n",
    "    PROMPT = row['Prompt']\n",
    "    GT = row['GT_Keyframe']\n",
    "    # search for similar images\n",
    "    clip_model.search_similar_images_top_3_clip(PROMPT, GT)\n",
    "    df_clip_llm_summary['Prompt'] = PROMPT\n",
    "    df_clip_llm_summary['GT_Keyframe'] = GT\n",
    "\n",
    "for i, res in enumerate(result):\n",
    "    df_clip_llm_summary[f'Top_{i + 1}'] = res\n",
    "\n",
    "# save on disk\n",
    "df_clip_llm_summary.to_csv('df_clip_llm_summary.csv', index=False)"
   ],
   "id": "94bea7410edfea44"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Embedded with CLIP - extensive_summary: OCR * Transcriptions * LLAVA \n",
    "# TODO: Generate Embeddings is not possible without the correct paths \"/Users/magic-rabbit/\" \n",
    "logger.info(f\" Embedded with CLIP - extensive_summary: OCR * Transcriptions * LLAVA\")\n",
    "\n",
    "clip_model.text_embeddings = None\n",
    "\n",
    "clip_model.img_paths = None\n",
    "\n",
    "clip_model.img_paths = extracted_data_path\n",
    "\n",
    "print(clip_model.img_paths)\n",
    "\n",
    "extracted_data_extensive_summary = [data[key]['llm_long_summary'] for key in data.keys() if\n",
    "                                    'llm_long_summary' in data[key]]\n",
    "logger.info(f\"Extracted_data llm_long_summary: {extracted_data_extensive_summary}\")\n",
    "\n",
    "embeddings = clip_model.generate_dataset_embeddings(extracted_data_extensive_summary)\n",
    "\n",
    "clip_model.text_embeddings = embeddings\n",
    "\n",
    "logger.info(f\"Clip_model.text_embeddings: {clip_model.text_embeddings}\")\n",
    "\n",
    "if isinstance(clip_model.text_embeddings, list):\n",
    "    for i, text_embedding in enumerate(clip_model.text_embeddings):\n",
    "        clip_model.text_embeddings[i] = torch.tensor(text_embedding)\n",
    "\n",
    "# create one single torch for sim search \n",
    "clip_model.text_embeddings = torch.stack(clip_model.text_embeddings, dim=0)\n",
    "\n",
    "for i, row in df.iterrows():\n",
    "    logger.info(row['Prompt'])\n",
    "    # get prompt \n",
    "    PROMPT = row['Prompt']\n",
    "    GT = row['GT_Keyframe']\n",
    "    # search for similar images\n",
    "    #clip_model.search_similar_images(PROMPT)\n",
    "    clip_model.search_similar_images_top_3_clip(PROMPT, GT)\n",
    "    df_clip_extensive_summary['Prompt'] = PROMPT\n",
    "    df_clip_extensive_summary['GT_Keyframe'] = GT\n",
    "\n",
    "for i, res in enumerate(result):\n",
    "    df_clip_extensive_summary[f'Top_{i + 1}'] = res\n",
    "\n",
    "# save on disk\n",
    "df_clip_extensive_summary.to_csv('df_clip_extensive_summary.csv', index=False)"
   ],
   "id": "4229c4ab7bd3f0ba"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "c9f3566130432766"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "8f9e810ced8b3982"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "f68a639265049b08"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "36b7d851b10032d2"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
