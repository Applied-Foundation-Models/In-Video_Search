{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2024-06-28T08:47:30.212867Z"
    }
   },
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "from src.video_preprocessing.download_videos.youtube_download import preprocess_video\n",
    "from src.video_preprocessing.scene_detection.scene_detect import detect_scenes\n",
    "from src.video_preprocessing.download_videos.download_utils import (\n",
    "    transcribe_audio_files,\n",
    "    extract_and_store_audio,\n",
    ")\n",
    "from src.ocr.pytesseract_image_to_text import extract_text_from_image\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from src.text_embedder.embedder import text_to_embedding_transformer\n",
    "\n",
    "from src.llm.ollama_implementation.ollama_experiment import (\n",
    "    prompt_llm_summary,\n",
    "    generate_caption_using_llava,\n",
    "    prompt_llm_extensive_summary,\n",
    ")\n",
    "from src.video_preprocessing.download_videos.download_utils import (\n",
    "    transcription_to_text,\n",
    "    create_metadata,\n",
    ")\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "from loguru import logger\n",
    "import pickle\n",
    "\n",
    "from src.clip.clip_model import CLIPEmbeddingsModel\n",
    "\n",
    "import tqdm\n",
    "from pathlib import Path\n",
    "import os"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": "clip_model = CLIPEmbeddingsModel()\n",
   "id": "b9a4ccee7af8042e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "# print pickle file \n",
    "with open(\"bio_3_3_th5.pickle\", \"rb\") as file:\n",
    "    data = pickle.load(file)\n",
    "\n",
    "logger.info(f\"new_pickle: {data}\")"
   ],
   "id": "ab2bc52ac0393355",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# set image paths for the experiments\n",
    "extracted_data_path = [data[key]['img_path'] for key in data.keys() if\n",
    "                       'img_path' in data[key]]\n",
    "\n",
    "clip_model.img_paths = None\n",
    "\n",
    "clip_model.img_paths = extracted_data_path"
   ],
   "id": "c3c80ef659c6347c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-28T08:47:21.577105Z",
     "start_time": "2024-06-28T08:47:21.540976Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for i in tqdm(range(len(clip_model.img_paths))):\n",
    "    print(clip_model.img_paths[i])"
   ],
   "id": "f20fc3bbb76e0ca4",
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tqdm' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[2], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m \u001B[43mtqdm\u001B[49m(\u001B[38;5;28mrange\u001B[39m(\u001B[38;5;28mlen\u001B[39m(clip_model\u001B[38;5;241m.\u001B[39mimg_paths))):\n\u001B[0;32m      2\u001B[0m     \u001B[38;5;28mprint\u001B[39m(clip_model\u001B[38;5;241m.\u001B[39mimg_paths[i])\n",
      "\u001B[1;31mNameError\u001B[0m: name 'tqdm' is not defined"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-28T08:42:28.709952Z",
     "start_time": "2024-06-28T08:42:28.440284Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Embedded with standard Tokenizer - short_llm_summary: OCR * Transcriptions * LLAVA \n",
    "# TODO: Need to get embedding with standard tokenizer for short_llm_summary\n",
    "\n",
    "\n",
    "logger.info(f\"Embedded with standard Tokenizer - clip_llm_summary: OCR * LLAVA * Transcriptions\")\n",
    "\n",
    "clip_model.text_embeddings = None\n",
    "\n",
    "# Assuming that the pickle file has standard tokenizer embeddings \n",
    "extracted_data_text = [data[key]['ocr_extracted_text'] for key in data.keys() if\n",
    "                       'ocr_extracted_text' in data[key]]\n",
    "\n",
    "extracted_data_transcriptions = [data[key]['transcription'] for key in data.keys() if\n",
    "                                 'transcription' in data[key]]\n",
    "\n",
    "extracted_data_llava_result = [data[key]['llava_result'] for key in data.keys() if\n",
    "                               'llava_result' in data[key]]\n",
    "\n",
    "# iterate through the ocr text and llava results lists\n",
    "from tqdm import tqdm\n",
    "\n",
    "keyframes = {}\n",
    "\n",
    "# iterate through the ocr text and llava results lists with a progress bar\n",
    "for i in tqdm(range(len(clip_model.img_paths))):\n",
    "    # only generate emb for the first 3 elements\n",
    "    if i <3:\n",
    "        ocr_text = extracted_data_text[i]\n",
    "        transcription = extracted_data_transcriptions[i]\n",
    "        llava_results = extracted_data_llava_result[i]\n",
    "        \n",
    "        short_llm_summary = prompt_llm_summary(\n",
    "            slide_content=ocr_text,\n",
    "            transcription=transcription,\n",
    "            llava_output=llava_results,\n",
    "        )\n",
    "    \n",
    "        extensive_summary = prompt_llm_extensive_summary(\n",
    "            slide_content=ocr_text,\n",
    "            transcription=transcription,\n",
    "            llava_output=llava_results,\n",
    "        )\n",
    "        \n",
    "        # these are the embeddings with a standard  tokenizer\n",
    "        standard_text_embedding = clip_model.generate_dataset_embeddings_standard_tokenizer(\n",
    "                short_llm_summary\n",
    "        )\n",
    "        extensive_text_embedding = clip_model.generate_dataset_embeddings_standard_tokenizer(\n",
    "                extensive_summary\n",
    "        )\n",
    "        keyframe_num = \"dummy\"\n",
    "        filepath = \"dummy\"\n",
    "        timestamps = \"dummy\"\n",
    "        clip_text_embedding = standard_text_embedding\n",
    "        clip_image_embedding = \"dummy\"\n",
    "        keyframe, keyframe_metadata = create_metadata(\n",
    "            keyframe_num,\n",
    "            filepath,\n",
    "            timestamps,\n",
    "            transcription,\n",
    "            ocr_text,\n",
    "            llava_results,\n",
    "            short_llm_summary,\n",
    "            extensive_summary,\n",
    "            standard_text_embedding,\n",
    "            extensive_text_embedding,\n",
    "        )\n",
    "        keyframes[keyframe] = keyframe_metadata\n",
    "        # print(keyframes)\n",
    "\n",
    "    # Save keyframes dictionary as Pickle\n",
    "\n",
    "# Save with pickle\n",
    "with open(\"data_standard_tokenizer.pickle\", \"wb\") as file:\n",
    "    pickle.dump(keyframes, file)\n"
   ],
   "id": "3141347d755e482f",
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'logger' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[1], line 5\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;66;03m# Embedded with standard Tokenizer - short_llm_summary: OCR * Transcriptions * LLAVA \u001B[39;00m\n\u001B[0;32m      2\u001B[0m \u001B[38;5;66;03m# TODO: Need to get embedding with standard tokenizer for short_llm_summary\u001B[39;00m\n\u001B[1;32m----> 5\u001B[0m \u001B[43mlogger\u001B[49m\u001B[38;5;241m.\u001B[39minfo(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mEmbedded with standard Tokenizer - clip_llm_summary: OCR * LLAVA * Transcriptions\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m      7\u001B[0m clip_model\u001B[38;5;241m.\u001B[39mtext_embeddings \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m      9\u001B[0m \u001B[38;5;66;03m# Assuming that the pickle file has standard tokenizer embeddings \u001B[39;00m\n",
      "\u001B[1;31mNameError\u001B[0m: name 'logger' is not defined"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "a7a11c66aa8bebc1"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
