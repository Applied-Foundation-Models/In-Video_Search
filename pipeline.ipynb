{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-17T10:12:18.212321Z",
     "start_time": "2024-06-17T10:11:54.365201Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "from src.video_preprocessing.download_videos.youtube_download import preprocess_video\n",
    "from src.video_preprocessing.scene_detection.scene_detect import detect_scenes\n",
    "from src.video_preprocessing.download_videos.download_utils import (\n",
    "    transcribe_audio_files,\n",
    "    extract_and_store_audio,\n",
    "    transcription_to_text,\n",
    "    create_metadata,\n",
    ")\n",
    "from src.ocr.pytesseract_image_to_text import extract_text_from_image\n",
    "from src.llm.ollama_implementation.ollama_experiment import (\n",
    "    prompt_llm_summary,\n",
    "    generate_caption_using_llava,\n",
    "    prompt_llm_extensive_summary,\n",
    "    extract_json\n",
    ")\n",
    "\n",
    "from transformers import CLIPProcessor, CLIPModel, CLIPTokenizer\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "from loguru import logger\n",
    "import pickle\n",
    "\n",
    "from src.clip.clip_model import CLIPEmbeddingsModel\n",
    "\n",
    "import tqdm\n",
    "from pathlib import Path"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download Pipeline\n",
    "Download a video from a specific URL on YouTube, then run:\n",
    "- Scene detection\n",
    "- Keyframe detection\n",
    "\n",
    "The resulting data will be stored under `/data/raw/<NAME>`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-17T10:12:18.215983Z",
     "start_time": "2024-06-17T10:12:18.215983Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define options and input for downloading a video from youtube\n",
    "\n",
    "# INSERT video name here\n",
    "name = \"biology_chapter_3_3_treshhold_5\"\n",
    "# INSERT video URL here\n",
    "url = \"https://youtu.be/DZSEErNZ1d4?si=f6YxKQ9rP6iqgTfk\"\n",
    "# INSERT chunk length in seconds 30s --> 30, no splitting: None\n",
    "chunks = None\n",
    "\n",
    "opts_aud = {\"format\": \"mp3/bestaudio/best\", \"keep-video\": True}\n",
    "opts_vid = {\"format\": \"mp4/bestvideo/best\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-17T08:31:11.939839Z",
     "start_time": "2024-06-17T08:31:09.105885Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-06-27 12:23:19.131\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.video_preprocessing.download_videos.youtube_download\u001b[0m:\u001b[36mpreprocess_video\u001b[0m:\u001b[36m49\u001b[0m - \u001b[1mStarting AutoCaptioning...\u001b[0m\n",
      "\u001b[32m2024-06-27 12:23:19.132\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.video_preprocessing.download_videos.youtube_download\u001b[0m:\u001b[36mpreprocess_video\u001b[0m:\u001b[36m50\u001b[0m - \u001b[1mResults will be stored in data/raw/biology_chapter_3_3_treshhold_5\u001b[0m\n",
      "\u001b[32m2024-06-27 12:23:19.133\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.video_preprocessing.download_videos.youtube_download\u001b[0m:\u001b[36mpreprocess_video\u001b[0m:\u001b[36m58\u001b[0m - \u001b[1mCreated chunks folders\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[youtube] Extracting URL: https://youtu.be/DZSEErNZ1d4?si=f6YxKQ9rP6iqgTfk\n",
      "[youtube] DZSEErNZ1d4: Downloading webpage\n",
      "[youtube] DZSEErNZ1d4: Downloading ios player API JSON\n",
      "[youtube] DZSEErNZ1d4: Downloading player a95aa57a\n",
      "[youtube] DZSEErNZ1d4: Downloading m3u8 information\n",
      "[info] DZSEErNZ1d4: Downloading 1 format(s): 18\n",
      "[download] Destination: /Users/haseeb/Desktop/Prak_New/afm-vlm/data/raw/biology_chapter_3_3_treshhold_5/biology_chapter_3_3_treshhold_5.mp4\n",
      "[download] 100% of   85.73MiB in 00:00:23 at 3.68MiB/s     \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-06-27 12:23:44.811\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.video_preprocessing.download_videos.youtube_download\u001b[0m:\u001b[36mpreprocess_video\u001b[0m:\u001b[36m91\u001b[0m - \u001b[1mVideo is not splitted:\u001b[0m\n",
      "\u001b[32m2024-06-27 12:23:44.812\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.video_preprocessing.download_videos.youtube_download\u001b[0m:\u001b[36mpreprocess_video\u001b[0m:\u001b[36m95\u001b[0m - \u001b[1mVideo downloaded successfully!\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Downloads the video creates the relevant datafolders and transcribes the video\n",
    "data_path = preprocess_video(\n",
    "    download=True,\n",
    "    uploaded_vid=\"ignore\",  # path to local file\n",
    "    url=url,\n",
    "    name=name,\n",
    "    aud_opts=opts_aud,\n",
    "    vid_opts=opts_vid,  # Video download settings\n",
    "    audio_file=name + \".mp3\",\n",
    "    input_file=name + \".mp4\",\n",
    "    output=\"output.mp4\",\n",
    "    split_length=chunks,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-17T08:41:53.940840Z",
     "start_time": "2024-06-17T08:31:11.939839Z"
    }
   },
   "outputs": [],
   "source": [
    "# Â Now that we have downloaded the video we want to perform scene_Detection:\n",
    "detect_scenes(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-17T08:43:22.576364Z",
     "start_time": "2024-06-17T08:41:53.940840Z"
    }
   },
   "outputs": [],
   "source": [
    "# Extract the audio per detected scene\n",
    "extract_and_store_audio(\n",
    "    os.path.join(data_path, \"scene_snippets\"),\n",
    "    os.path.join(data_path, \"audio_chunks\"),\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Audio Transcription using Whisper\n",
    "\n",
    "For Faster Inference Please Use Tiny!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-17T08:44:28.464003Z",
     "start_time": "2024-06-17T08:44:22.751722Z"
    }
   },
   "outputs": [],
   "source": [
    "# Transcribe the different snippets:\n",
    "audio_dir = os.path.join(data_path, \"audio_chunks\")\n",
    "transcriptions_dir = os.path.join(data_path, \"transcriptions\")\n",
    "\n",
    "model_type = \"tiny\"  # change to 'large' if you want more accurate results,\n",
    "# change to 'medium.en' or 'large.en' for all english language tasks,\n",
    "# and change to 'small' or 'base' for faster inference\n",
    "lang = \"en\"\n",
    "\n",
    "# Run whisper on all .wav files in audio_dir\n",
    "transcribe_audio_files(audio_dir, transcriptions_dir, model_type=model_type, lang=lang)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the CLIP Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-17T10:16:04.804524Z",
     "start_time": "2024-06-17T10:16:02.762522Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 800x2000 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# create instance\n",
    "clip_model = CLIPEmbeddingsModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-17T10:41:02.487038Z",
     "start_time": "2024-06-17T10:41:02.392682Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-06-28 11:08:16.937\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m15\u001b[0m - \u001b[1mImage_dataset: [<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=482x360 at 0x16B3EC340>, <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=482x360 at 0x16B3EDF90>, <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=482x360 at 0x16B3EDFF0>, <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=482x360 at 0x16B3EC3A0>, <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=482x360 at 0x16B3EDFC0>, <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=482x360 at 0x16B3EDEA0>, <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=482x360 at 0x16B3EE020>, <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=482x360 at 0x16B3EE080>, <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=482x360 at 0x16B3EE0E0>, <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=482x360 at 0x16B3EE140>, <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=482x360 at 0x16B3EE1A0>, <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=482x360 at 0x16B3EE200>, <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=482x360 at 0x16B3EE260>, <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=482x360 at 0x16B3EE2C0>, <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=482x360 at 0x16B3EE320>, <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=482x360 at 0x16B3EE380>, <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=482x360 at 0x16B3EE3E0>, <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=482x360 at 0x16B3EE440>, <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=482x360 at 0x16B3EE4A0>, <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=482x360 at 0x16B3EE500>, <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=482x360 at 0x16B3EE560>, <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=482x360 at 0x16B3EE5C0>, <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=482x360 at 0x16B3EE620>, <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=482x360 at 0x16B3EE680>, <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=482x360 at 0x16B3EE6E0>, <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=482x360 at 0x16B3EE740>, <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=482x360 at 0x16B3EE7A0>, <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=482x360 at 0x16B3EE800>, <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=482x360 at 0x16B3EE860>, <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=482x360 at 0x16B3EE8C0>, <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=482x360 at 0x16B3EE920>, <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=482x360 at 0x16B3EE980>, <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=482x360 at 0x16B3EE9E0>, <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=482x360 at 0x16B3EEA40>, <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=482x360 at 0x16B3EEAA0>, <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=482x360 at 0x16B3EEB00>, <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=482x360 at 0x16B3EEB60>, <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=482x360 at 0x16B3EEBC0>, <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=482x360 at 0x16B3EEC20>, <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=482x360 at 0x16B3EEC80>, <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=482x360 at 0x16B3EECE0>, <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=482x360 at 0x16B3EED40>, <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=482x360 at 0x16B3EEDA0>, <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=482x360 at 0x16B3EEE00>, <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=482x360 at 0x16B3EEE60>, <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=482x360 at 0x16B3EEEC0>, <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=482x360 at 0x16B3EEF20>, <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=482x360 at 0x16B3EEF80>, <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=482x360 at 0x16B3EEFE0>, <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=482x360 at 0x16B3EF040>, <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=482x360 at 0x16B3EF0A0>, <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=482x360 at 0x16B3EF100>, <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=482x360 at 0x16B3EF160>, <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=482x360 at 0x16B3EF1C0>, <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=482x360 at 0x16B3EF220>, <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=482x360 at 0x16B3EF280>, <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=482x360 at 0x16B3EF2E0>, <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=482x360 at 0x16B3EF340>, <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=482x360 at 0x16B3EF3A0>, <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=482x360 at 0x16B3EF400>, <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=482x360 at 0x16B3EF460>, <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=482x360 at 0x16B3EF4C0>, <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=482x360 at 0x16B3EF520>, <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=482x360 at 0x16B3EF580>, <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=482x360 at 0x16B3EF5E0>, <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=482x360 at 0x16B3EF640>, <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=482x360 at 0x16B3EF6A0>, <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=482x360 at 0x16B3EF700>, <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=482x360 at 0x16B3EF760>, <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=482x360 at 0x16B3EF7C0>, <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=482x360 at 0x16B3EF820>, <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=482x360 at 0x16B3EF880>, <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=482x360 at 0x16B3EF8E0>, <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=482x360 at 0x16B3EF940>, <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=482x360 at 0x16B3EF9A0>, <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=482x360 at 0x16B3EFA00>, <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=482x360 at 0x16B3EFA60>, <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=482x360 at 0x16B3EFAC0>, <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=482x360 at 0x16B3EFB20>, <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=482x360 at 0x16B3EFB80>, <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=482x360 at 0x16B3EFBE0>, <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=482x360 at 0x16B3EFC40>, <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=482x360 at 0x16B3EFCA0>, <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=482x360 at 0x16B3EFD00>, <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=482x360 at 0x16B3EFD60>]\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# get current directory\n",
    "# Get the path of the current notebook\n",
    "notebook_path = Path().resolve()\n",
    "image_path = os.path.join(notebook_path, \"data\", \"raw\", name, \"extracted_keyframes\")\n",
    "\n",
    "images = []\n",
    "# make a list out of the images\n",
    "for image in os.listdir(image_path):\n",
    "    if image.endswith(\".jpg\"):\n",
    "        images.append(os.path.join(image_path, image))\n",
    "\n",
    "# load and process the dataset\n",
    "image_dataset = clip_model.load_and_process_dataset(images)\n",
    "\n",
    "logger.info(f\"Image_dataset: {image_dataset}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Starting the Analysis of the Information Contained in the Video\n",
    "\n",
    "### Inputs\n",
    "\n",
    "* **Transcriptions**: [insert description or link to transcription]\n",
    "* **Extraction from Slides using OCR**: [insert description or link to extracted content]\n",
    "* **Textual Interpretation of Visual Information using LLAVA**: [insert description or link to \n",
    "interpreted information]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-17T21:02:14.765583Z",
     "start_time": "2024-06-17T21:02:10.955208Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/87 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running...\n",
      "transcriptions...\n",
      "\n",
      "Transport Across Membranes\n",
      "\n",
      "+ Exocytosis: a membrane-bound vesicle fuses with the\n",
      "membrane and expels the large molecule outside the cell\n",
      "\n",
      "(2) Exocytosis\n",
      "\n",
      "\n",
      "this is talking about cell structure\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/87 [04:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "extensive: \n",
      "\n",
      "Here is the combined summary in JSON format:\n",
      "\n",
      "{\n",
      "\"Summary\": \"Transport Across Membranes: The lecture discussed transport across membranes, specifically exocytosis, where a membrane-bound vesicle fuses with the membrane and expels large molecules outside the cell. This process is important for cellular communication and waste removal.\"\n",
      "}\n",
      "Transport Across Membranes: The lecture discussed transport across membranes, specifically exocytosis, where a membrane-bound vesicle fuses with the membrane and expels large molecules outside the cell. This process is important for cellular communication and waste removal.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import tqdm\n",
    "\n",
    "# Transform transcription file\n",
    "keyframes = {}\n",
    "ocr_extracted_text = []\n",
    "\n",
    "timestamp_file_path = os.path.join(\n",
    "    os.path.dirname(image_path), \"extracted_keyframes\", name + \"-Scenes.csv\"\n",
    ")\n",
    "\n",
    "for filename in tqdm.tqdm(os.listdir(image_path)):\n",
    "    # Check if the file ends with the specified extension\n",
    "    print('running...')\n",
    "    if filename.endswith(\".jpg\"):\n",
    "        filepath = os.path.join(image_path, filename)\n",
    "        keyframe_num = int(filename.split(\"-\")[2])\n",
    "\n",
    "        transcription_file_path = os.path.join(\n",
    "            os.path.dirname(image_path),\n",
    "            \"transcriptions\",\n",
    "            filename.replace(\"-01.jpg\", \".csv\"),\n",
    "        )\n",
    "\n",
    "        transcription, timestamps = transcription_to_text(\n",
    "            keyframe_num, transcription_file_path, timestamp_file_path\n",
    "        )\n",
    "        # logger.info(f\"Transcription_text: {transcription}\")\n",
    "\n",
    "        # Extract text using OCR:\n",
    "        ocr_text = extract_text_from_image(filepath)\n",
    "        # logger.info(f\"OCR_results: {ocr_text}\")\n",
    "\n",
    "       #Extract textual understanding of Visual features using LLAVA:\n",
    "        #llava_results = generate_caption_using_llava(filepath)\n",
    "        # logger.info(f\"LLava_results: {llava_results}\")\n",
    "\n",
    "        \n",
    "        #DUMMY Llava results\n",
    "        llava_results = 'this is talking about cell structure'\n",
    "        \n",
    "        try:\n",
    "            clip_llm_summary = prompt_llm_summary(\n",
    "                slide_content=ocr_text,\n",
    "                transcription=transcription,\n",
    "                llava_output=llava_results,\n",
    "            )\n",
    "            clip_llm_summary = extract_json(clip_llm_summary)\n",
    "\n",
    "            extensive_summary = prompt_llm_extensive_summary(\n",
    "                slide_content=ocr_text,\n",
    "                transcription=transcription,\n",
    "                llava_output=llava_results,\n",
    "            )\n",
    "            extensive_summary = extract_json(extensive_summary)\n",
    "        except KeyError:\n",
    "            print('Output summaries not in proper format')\n",
    "        \n",
    "        \n",
    "        break\n",
    "\n",
    "        # # Alternative that goes faster.\n",
    "        # ocr_text = \"ocr_text\"\n",
    "        # llava_results = \"llava_results\"\n",
    "        # clip_llm_summary = \"ontrary to popular belief, Lorem Ipsum is not simply random text. It has roots in a piece of classical Latin literature from 45 BC, making it over 2000 years old. Richard McClintock, a Latin professor at Hampden-Sydney College in Virginia, looked up one of the more obscure Latin words, consectetur, from a Lorem Ipsum passage, and going through the cites of the word in classical literature, discovered the undoubtable source. Lorem Ipsum\"\n",
    "        # extensive_summary = \"extensive_summary\"\n",
    "\n",
    "        # generate embeddings\n",
    "        opened_image = Image.open(filepath)\n",
    "        \n",
    "\n",
    "        # embeddings = clip_model.generate_image_embeddings(\n",
    "        #     clip_llm_summary, opened_image\n",
    "        # )\n",
    "        # clip_text_embedding = embeddings[\"text_embeds\"]\n",
    "        # clip_image_embedding = embeddings[\"image_embeds\"]\n",
    "\n",
    "        # keyframe, keyframe_metadata = create_metadata(\n",
    "        #     keyframe_num,\n",
    "        #     filepath,\n",
    "        #     timestamps,\n",
    "        #     transcription,\n",
    "        #     ocr_text,\n",
    "        #     llava_results,\n",
    "        #     clip_llm_summary,\n",
    "        #     extensive_summary,\n",
    "        #     clip_text_embedding,\n",
    "        #     clip_image_embedding,\n",
    "        # )\n",
    "        # keyframes[keyframe] = keyframe_metadata\n",
    "        # print(keyframes)\n",
    "\n",
    "    # Save keyframes dictionary as Pickle\n",
    "\n",
    "# Save with pickle\n",
    "# with open(\"bio_3_3_th5.pickle\", \"wb\") as file:\n",
    "#     pickle.dump(keyframes, file)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test 1: Search for exact similar Text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_image_path = os.path.join(\n",
    "    base_dir,\n",
    "    \"data\",\n",
    "    \"raw\",\n",
    "    \"biology_chapter_3_3\",\n",
    "    \"extracted_keyframes\",\n",
    "    \"biology_chapter_3_3-Scene-097-01.jpg\",\n",
    ")\n",
    "\n",
    "test_text_description = extract_text_from_image(test_image_path)\n",
    "# Search for similar images in database\n",
    "clip_model.search_similar_images(test_text_description)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test 2: Search for for a slightly different Text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_text = \"plasma membrane and stuff going on\"\n",
    "\n",
    "clip_model.search_similar_images(query_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #######OLD\n",
    "\n",
    "\n",
    "# # Generated one embedding?\n",
    "# def get_model_info(model_ID, device):\n",
    "#     # Save the model to device\n",
    "#     model = CLIPModel.from_pretrained(model_ID).to(device)\n",
    "#     # Get the processor\n",
    "#     processor = CLIPProcessor.from_pretrained(model_ID)\n",
    "#     # Get the tokenizer\n",
    "#     tokenizer = CLIPTokenizer.from_pretrained(model_ID)\n",
    "#     # Return model, processor & tokenizer\n",
    "#     return model, processor, tokenizer\n",
    "\n",
    "\n",
    "# # Set the device\n",
    "# device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "# # Define the model ID\n",
    "# model_ID = \"openai/clip-vit-base-patch32\"\n",
    "# # Get model, processor & tokenizer\n",
    "# model, processor, tokenizer = get_model_info(model_ID, device)\n",
    "\n",
    "\n",
    "# def get_image(image_path):\n",
    "#     image = Image.open(image_path)\n",
    "#     # Convert the image to RGB\n",
    "#     rgb_image = image.convert(\"RGB\")\n",
    "#     return rgb_image\n",
    "\n",
    "\n",
    "# def get_single_image_embedding(text, my_image, processor, model, device):\n",
    "#     image = processor(text=text, images=my_image, return_tensors=\"pt\")[\n",
    "#         \"pixel_values\"\n",
    "#     ].to(device)\n",
    "#     embedding = model.get_image_features(image)\n",
    "#     # convert the embeddings to numpy array\n",
    "#     return embedding.cpu().detach().numpy()\n",
    "\n",
    "\n",
    "# one_image = get_image(\n",
    "#     image_path=\"/Users/magic-rabbit/Documents/AFM/afm-vlm/data/raw/biology_chapter_3_3/extracted_keyframes/biology_chapter_3_3-Scene-055-01.jpg\"\n",
    "# )\n",
    "\n",
    "# one_vector = get_single_image_embedding(\n",
    "#     response, one_image, processor, model, device\n",
    "# )  # Simple test\n",
    "# print(one_vector)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "afm-vlm-JEfTnAR4-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
