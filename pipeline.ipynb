{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-10T10:59:49.546379Z",
     "start_time": "2024-06-10T10:59:45.505917Z"
    }
   },
   "source": [
    "import os\n",
    "from src.video_preprocessing.download_videos.youtube_download import preprocess_video\n",
    "from src.video_preprocessing.scene_detection.scene_detect import detect_scenes\n",
    "from src.video_preprocessing.scene_detection.ocr import extract_text_from_slide\n",
    "from src.video_preprocessing.download_videos.download_utils import (\n",
    "    transcribe_audio_files,\n",
    "    extract_and_store_audio,\n",
    "    transcription_to_text,\n",
    ")\n",
    "from src.ocr.pytesseract_image_to_text import extract_text_from_image\n",
    "from src.llm.ollama_implementation.ollama_experiment import (\n",
    "    prompt_llm_summary,\n",
    "    generate_caption_using_llava,\n",
    ")\n",
    "from transformers import CLIPProcessor, CLIPModel, CLIPTokenizer\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import pandas as pd\n",
    "import time\n",
    "from loguru import logger"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[32m2024-06-10 12:59:48.341\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.ocr.pytesseract_image_to_text\u001B[0m:\u001B[36m<module>\u001B[0m:\u001B[36m17\u001B[0m - \u001B[1mExtracted text: Lecture overview\n",
      "\n",
      "1 R programming basics\n",
      "\n",
      "1. Get 2 Data wrangling\n",
      "3 Tidy data\n",
      "2. Look 4 Low dimensional visualization\n",
      "5 High dimensi i\n",
      "3. Conclude 7 Empirical Statistical Assessment\n",
      "\n",
      "8 Analytical Statistical Assessment\n",
      "9 Statistical Assessment for Big Data\n",
      "Case Study\n",
      "10 Linear regression\n",
      "11 Classification\n",
      "12 Supervised Learning\n",
      "\n",
      "Julien Gagneur Graphically supported hypotheses 3/70\n",
      "\n",
      "\u001B[0m\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download Pipeline\n",
    "Download a video from a specific URL on YouTube, then run:\n",
    "- Scene detection\n",
    "- Keyframe detection\n",
    "\n",
    "The resulting data will be stored under `/data/raw/<NAME>`"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-10T10:59:52.241372Z",
     "start_time": "2024-06-10T10:59:52.223278Z"
    }
   },
   "source": [
    "# Define options and input for downloading a video from youtube\n",
    "\n",
    "# INSERT video name here\n",
    "name = \"biology_chapter_3_3\"\n",
    "# INSERT video URL here\n",
    "url = \"https://youtu.be/DZSEErNZ1d4?si=f6YxKQ9rP6iqgTfk\"\n",
    "# INSERT chunk length in seconds 30s --> 30, no splitting: None\n",
    "chunks = None\n",
    "\n",
    "opts_aud = {\"format\": \"mp3/bestaudio/best\", \"keep-video\": True}\n",
    "opts_vid = {\"format\": \"mp4/bestvideo/best\"}"
   ],
   "outputs": [],
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-10T10:59:55.680358Z",
     "start_time": "2024-06-10T10:59:53.779941Z"
    }
   },
   "source": [
    "# Downloads the video creates the relevant datafolders and transcribes the video\n",
    "data_path = preprocess_video(\n",
    "    download=True,\n",
    "    uploaded_vid=\"ignore\",  # path to local file\n",
    "    url=url,\n",
    "    name=name,\n",
    "    aud_opts=opts_aud,\n",
    "    vid_opts=opts_vid,  # Video download settings\n",
    "    audio_file=name + \".mp3\",\n",
    "    input_file=name + \".mp4\",\n",
    "    output=\"output.mp4\",\n",
    "    split_length=chunks,\n",
    ")"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[32m2024-06-10 12:59:53.782\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.video_preprocessing.download_videos.youtube_download\u001B[0m:\u001B[36mpreprocess_video\u001B[0m:\u001B[36m49\u001B[0m - \u001B[1mStarting AutoCaptioning...\u001B[0m\n",
      "\u001B[32m2024-06-10 12:59:53.783\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.video_preprocessing.download_videos.youtube_download\u001B[0m:\u001B[36mpreprocess_video\u001B[0m:\u001B[36m50\u001B[0m - \u001B[1mResults will be stored in data/raw/biology_chapter_3_3\u001B[0m\n",
      "\u001B[32m2024-06-10 12:59:53.785\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.video_preprocessing.download_videos.youtube_download\u001B[0m:\u001B[36mpreprocess_video\u001B[0m:\u001B[36m58\u001B[0m - \u001B[1mCreated chunks folders\u001B[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[youtube] Extracting URL: https://youtu.be/DZSEErNZ1d4?si=f6YxKQ9rP6iqgTfk\n",
      "[youtube] DZSEErNZ1d4: Downloading webpage\n",
      "[youtube] DZSEErNZ1d4: Downloading ios player API JSON\n",
      "[youtube] DZSEErNZ1d4: Downloading m3u8 information\n",
      "[info] DZSEErNZ1d4: Downloading 1 format(s): 22\n",
      "[download] C:\\Users\\baatout\\PycharmProjects\\afm-vlm\\data\\raw\\biology_chapter_3_3\\biology_chapter_3_3.mp4 has already been downloaded\n",
      "[download] 100% of  126.17MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[32m2024-06-10 12:59:55.663\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.video_preprocessing.download_videos.youtube_download\u001B[0m:\u001B[36mpreprocess_video\u001B[0m:\u001B[36m91\u001B[0m - \u001B[1mVideo is not splitted:\u001B[0m\n",
      "\u001B[32m2024-06-10 12:59:55.664\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.video_preprocessing.download_videos.youtube_download\u001B[0m:\u001B[36mpreprocess_video\u001B[0m:\u001B[36m95\u001B[0m - \u001B[1mVideo downloaded successfully!\u001B[0m\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2024-06-10T07:49:42.314242Z"
    }
   },
   "source": [
    "#  Now that we have downloaded the video we want to perform scene_Detection:\n",
    "detect_scenes(data_path)"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[32m2024-06-10 09:49:42.322\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.video_preprocessing.scene_detection.scene_detect\u001B[0m:\u001B[36mdetect_scenes\u001B[0m:\u001B[36m29\u001B[0m - \u001B[1mFound file\u001B[0m\n",
      "\u001B[32m2024-06-10 09:49:42.322\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.video_preprocessing.scene_detection.scene_detect\u001B[0m:\u001B[36mdetect_scenes\u001B[0m:\u001B[36m33\u001B[0m - \u001B[1mName:biology_chapter_3_3.mp4,dirname:C:\\Users\\baatout\\PycharmProjects\\afm-vlm\\data/raw\\biology_chapter_3_3\\biology_chapter_3_3.mp4\u001B[0m\n",
      "\u001B[32m2024-06-10 09:49:42.322\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.video_preprocessing.scene_detection.scene_detect\u001B[0m:\u001B[36mdetect_scenes\u001B[0m:\u001B[36m35\u001B[0m - \u001B[1mRunning scene_detection:\u001B[0m\n"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the audio per detected scene\n",
    "extract_and_store_audio(\n",
    "    os.path.join(data_path, \"scene_snippets\"),\n",
    "    os.path.join(data_path, \"audio_chunks\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Audio Transcription using Whisper\n",
    "\n",
    "For Faster Inference Please Use Tiny!"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-10T11:04:28.278181Z",
     "start_time": "2024-06-10T11:00:01.199766Z"
    }
   },
   "source": [
    "# Transcribe the different snippets snippets:\n",
    "audio_dir = os.path.join(data_path, \"audio_chunks\")\n",
    "transcriptions_dir = os.path.join(data_path, \"transcriptions\")\n",
    "\n",
    "model_type = \"tiny\"  # change to 'large' if you want more accurate results,\n",
    "# change to 'medium.en' or 'large.en' for all english language tasks,\n",
    "# and change to 'small' or 'base' for faster inference\n",
    "lang = \"en\"\n",
    "\n",
    "# Run whisper on all .wav files in audio_dir\n",
    "transcribe_audio_files(audio_dir, transcriptions_dir, model_type=model_type, lang=lang)"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[32m2024-06-10 13:00:01.204\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.video_preprocessing.download_videos.download_utils\u001B[0m:\u001B[36mtranscribe_audio_files\u001B[0m:\u001B[36m361\u001B[0m - \u001B[1mStarting pooling:\u001B[0m\n",
      "100%|██████████| 109/109 [04:26<00:00,  2.44s/it]\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Starting the Analysis of the Information Contained in the Video\n",
    "\n",
    "### Inputs\n",
    "\n",
    "* **Transcriptions**: [insert description or link to transcription]\n",
    "* **Extraction from Slides using OCR**: [insert description or link to extracted content]\n",
    "* **Textual Interpretation of Visual Information using LLAVA**: [insert description or link to \n",
    "interpreted information]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[32m2024-06-07 19:28:50.613\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36m__main__\u001B[0m:\u001B[36m<module>\u001B[0m:\u001B[36m27\u001B[0m - \u001B[1mElapsed Time: 42.34112215042114 seconds\u001B[0m\n",
      "\u001B[32m2024-06-07 19:28:50.617\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36m__main__\u001B[0m:\u001B[36m<module>\u001B[0m:\u001B[36m31\u001B[0m - \u001B[1mLLM_Summary: Here is a summary of the lecture content:\n",
      "\n",
      "**Slide Summary:**\n",
      "The slide discusses micronutrients, specifically vitamins, which are organic substances that help speed up chemical reactions in the body. Most vitamins cannot be synthesized by the body and must be obtained from food. Vitamin D is an exception, as it can be synthesized with sunlight. The slide highlights the importance of micronutrients for maintaining bodily functions.\n",
      "\n",
      "**Key Topics:**\n",
      "Vitamins, micronutrients, organic substances, coenzymes, vitamin deficiencies, cancer prevention, heart disease prevention, aging process, sunlight, vitamin D synthesis, supplementation.\n",
      "\n",
      "**Queryable Information:**\n",
      "Tags: Vitamins, Micronutrients, Organic Substances, Coenzymes, Vitamin Deficiencies, Cancer Prevention, Heart Disease Prevention, Aging Process, Sunlight, Vitamin D Synthesis, Supplementation.\n",
      "Categories: Nutrition, Health, Biology\n",
      "Specific Concepts: Vitamin Functionality, Chemical Reactions, Body Functions, Nutrient Deficiencies.\u001B[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is a summary of the lecture content:\n",
      "\n",
      "**Slide Summary:**\n",
      "The slide discusses micronutrients, specifically vitamins, which are organic substances that help speed up chemical reactions in the body. Most vitamins cannot be synthesized by the body and must be obtained from food. Vitamin D is an exception, as it can be synthesized with sunlight. The slide highlights the importance of micronutrients for maintaining bodily functions.\n",
      "\n",
      "**Key Topics:**\n",
      "Vitamins, micronutrients, organic substances, coenzymes, vitamin deficiencies, cancer prevention, heart disease prevention, aging process, sunlight, vitamin D synthesis, supplementation.\n",
      "\n",
      "**Queryable Information:**\n",
      "Tags: Vitamins, Micronutrients, Organic Substances, Coenzymes, Vitamin Deficiencies, Cancer Prevention, Heart Disease Prevention, Aging Process, Sunlight, Vitamin D Synthesis, Supplementation.\n",
      "Categories: Nutrition, Health, Biology\n",
      "Specific Concepts: Vitamin Functionality, Chemical Reactions, Body Functions, Nutrient Deficiencies.\n"
     ]
    }
   ],
   "source": [
    "transcription_file_path = \"/Users/magic-rabbit/Documents/AFM/afm-vlm/data/raw/biology_chapter_3_3/transcriptions/biology_chapter_3_3-Scene-055.csv\"\n",
    "image_path = \"/Users/magic-rabbit/Documents/AFM/afm-vlm/data/raw/biology_chapter_3_3/extracted_keyframes/biology_chapter_3_3-Scene-055-01.jpg\"\n",
    "\n",
    "start_time = time.time()\n",
    "# Transform transcription file\n",
    "transcription = transcription_to_text(transcription_file_path)\n",
    "logger.info(f\"Transcription_text: {transcription}\")\n",
    "\n",
    "# Extract text using OCR:\n",
    "ocr_extracted_text = extract_text_from_image(image_path)\n",
    "logger.info(f\"OCR_results: {ocr_extracted_text}\")\n",
    "\n",
    "# Extract textual understanding of Visual features using LLAVA:\n",
    "\n",
    "llava_results = generate_caption_using_llava(image_path)\n",
    "logger.info(f\"LLava_results: {llava_results}\")\n",
    "\n",
    "response = prompt_llm_summary(\n",
    "    slide_content=ocr_extracted_text,\n",
    "    transcription=transcription,\n",
    "    llava_output=llava_results,\n",
    ")\n",
    "# Calculate the elapsed time\n",
    "elapsed_time = time.time() - start_time\n",
    "\n",
    "# Print the elapsed time\n",
    "logger.info(f\"Elapsed Time: {elapsed_time} seconds\")\n",
    "\n",
    "# print the resposne of the Slide:\n",
    "logger.info(f\"LLM_Summary: {response}\")"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-10T11:04:31.669925Z",
     "start_time": "2024-06-10T11:04:28.281267Z"
    }
   },
   "source": [
    "def get_model_info(model_ID, device):\n",
    "    # Save the model to device\n",
    "    model = CLIPModel.from_pretrained(model_ID).to(device)\n",
    "    # Get the processor\n",
    "    processor = CLIPProcessor.from_pretrained(model_ID)\n",
    "    # Get the tokenizer\n",
    "    tokenizer = CLIPTokenizer.from_pretrained(model_ID)\n",
    "    # Return model, processor & tokenizer\n",
    "    return model, processor, tokenizer\n",
    "\n",
    "\n",
    "# Set the device\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "# Define the model ID\n",
    "model_ID = \"openai/clip-vit-base-patch32\"\n",
    "# Get model, processor & tokenizer\n",
    "model, processor, tokenizer = get_model_info(model_ID, device)\n",
    "\n",
    "\n",
    "def get_image(image_path):\n",
    "    image = Image.open(image_path)\n",
    "    # Convert the image to RGB\n",
    "    rgb_image = image.convert(\"RGB\")\n",
    "    return rgb_image\n",
    "\n",
    "\n",
    "def get_single_image_embedding(text, my_image, processor, model, device):\n",
    "    image = processor(text=text, images=my_image, return_tensors=\"pt\")[\n",
    "        \"pixel_values\"\n",
    "    ].to(device)\n",
    "    embedding = model.get_image_features(image)\n",
    "    # convert the embeddings to numpy array\n",
    "    return embedding.cpu().detach().numpy()\n",
    "\n",
    "\n",
    "one_image = get_image(\n",
    "    image_path=\"/Users/magic-rabbit/Documents/AFM/afm-vlm/data/raw/biology_chapter_3_3/extracted_keyframes/biology_chapter_3_3-Scene-055-01.jpg\"\n",
    ")\n",
    "\n",
    "one_vector = get_single_image_embedding(\n",
    "    response, one_image, processor, model, device\n",
    ")  # Simple test"
   ],
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'C:\\\\Users\\\\magic-rabbit\\\\Documents\\\\AFM\\\\afm-vlm\\\\data\\\\raw\\\\biology_chapter_3_3\\\\extracted_keyframes\\\\biology_chapter_3_3-Scene-055-01.jpg'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[12], line 36\u001B[0m\n\u001B[0;32m     32\u001B[0m     \u001B[38;5;66;03m# convert the embeddings to numpy array\u001B[39;00m\n\u001B[0;32m     33\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m embedding\u001B[38;5;241m.\u001B[39mcpu()\u001B[38;5;241m.\u001B[39mdetach()\u001B[38;5;241m.\u001B[39mnumpy()\n\u001B[1;32m---> 36\u001B[0m one_image \u001B[38;5;241m=\u001B[39m \u001B[43mget_image\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m     37\u001B[0m \u001B[43m    \u001B[49m\u001B[43mimage_path\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43m/Users/magic-rabbit/Documents/AFM/afm-vlm/data/raw/biology_chapter_3_3/extracted_keyframes/biology_chapter_3_3-Scene-055-01.jpg\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\n\u001B[0;32m     38\u001B[0m \u001B[43m)\u001B[49m\n\u001B[0;32m     40\u001B[0m one_vector \u001B[38;5;241m=\u001B[39m get_single_image_embedding(\n\u001B[0;32m     41\u001B[0m     response, one_image, processor, model, device\n\u001B[0;32m     42\u001B[0m )  \u001B[38;5;66;03m# Simple test\u001B[39;00m\n",
      "Cell \u001B[1;32mIn[12], line 21\u001B[0m, in \u001B[0;36mget_image\u001B[1;34m(image_path)\u001B[0m\n\u001B[0;32m     20\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mget_image\u001B[39m(image_path):\n\u001B[1;32m---> 21\u001B[0m     image \u001B[38;5;241m=\u001B[39m \u001B[43mImage\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mopen\u001B[49m\u001B[43m(\u001B[49m\u001B[43mimage_path\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     22\u001B[0m     \u001B[38;5;66;03m# Convert the image to RGB\u001B[39;00m\n\u001B[0;32m     23\u001B[0m     rgb_image \u001B[38;5;241m=\u001B[39m image\u001B[38;5;241m.\u001B[39mconvert(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mRGB\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[1;32m~\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\afm-vlm-y3Vw7rqv-py3.10\\lib\\site-packages\\PIL\\Image.py:3277\u001B[0m, in \u001B[0;36mopen\u001B[1;34m(fp, mode, formats)\u001B[0m\n\u001B[0;32m   3274\u001B[0m     filename \u001B[38;5;241m=\u001B[39m os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39mrealpath(os\u001B[38;5;241m.\u001B[39mfspath(fp))\n\u001B[0;32m   3276\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m filename:\n\u001B[1;32m-> 3277\u001B[0m     fp \u001B[38;5;241m=\u001B[39m \u001B[43mbuiltins\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mopen\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfilename\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mrb\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m   3278\u001B[0m     exclusive_fp \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[0;32m   3280\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n",
      "\u001B[1;31mFileNotFoundError\u001B[0m: [Errno 2] No such file or directory: 'C:\\\\Users\\\\magic-rabbit\\\\Documents\\\\AFM\\\\afm-vlm\\\\data\\\\raw\\\\biology_chapter_3_3\\\\extracted_keyframes\\\\biology_chapter_3_3-Scene-055-01.jpg'"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-10T11:04:31.672532Z",
     "start_time": "2024-06-10T11:04:31.672532Z"
    }
   },
   "source": [
    "# Generated one embedding?\n",
    "\n",
    "print(one_vector)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Embeddings Generation using CLIP\n",
    "\n",
    "### Inputs\n",
    "\n",
    "* **Keyframes (images)**\n",
    "* **Extraction from Slides using OCR**\n"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-10T11:23:53.199842Z",
     "start_time": "2024-06-10T11:23:50.643375Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from src.clip.clip import CLIP\n",
    "\n",
    "clip = CLIP()"
   ],
   "outputs": [],
   "execution_count": 41
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-10T11:23:53.215465Z",
     "start_time": "2024-06-10T11:23:53.201841Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "#from src.clip.load_images import load_images_from_path\n",
    "from loguru import logger\n",
    "\n",
    "base_dir = os.path.dirname(os.path.abspath(\".\"))\n",
    "\n",
    "relative_image_path_1 = os.path.join(base_dir, 'afm-vlm', 'data', 'raw', 'biology_chapter_3_3', 'extracted_keyframes',\n",
    "                                     'biology_chapter_3_3-Scene-039-01.jpg')\n",
    "relative_image_path_2 = os.path.join(base_dir, 'afm-vlm', 'data', 'raw', 'biology_chapter_3_3', 'extracted_keyframes',\n",
    "                                     'biology_chapter_3_3-Scene-099-01.jpg')\n",
    "relative_image_path_3 = os.path.join(base_dir, 'afm-vlm', 'data', 'raw', 'biology_chapter_3_3', 'extracted_keyframes',\n",
    "                                     'biology_chapter_3_3-Scene-014-01.jpg')\n",
    "\n",
    "image_paths = [relative_image_path_1, relative_image_path_2, relative_image_path_3]\n",
    "\n",
    "def load_images_from_path(image_paths):\n",
    "    images = []\n",
    "    for path in image_paths:\n",
    "        images.append(Image.open(path))\n",
    "    return images\n",
    "\n",
    "image_dataset = load_images_from_path(image_paths)\n",
    "\n",
    "logger.info(f\"Image_dataset: {image_dataset}\")"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[32m2024-06-10 13:23:53.209\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36m__main__\u001B[0m:\u001B[36m<module>\u001B[0m:\u001B[36m25\u001B[0m - \u001B[1mImage_dataset: [<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=966x720 at 0x1DA369C3190>, <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=966x720 at 0x1DA369C13F0>, <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=966x720 at 0x1DA369C26E0>]\u001B[0m\n"
     ]
    }
   ],
   "execution_count": 42
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Generate OCR Captions"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-10T11:23:54.871685Z",
     "start_time": "2024-06-10T11:23:53.217465Z"
    }
   },
   "cell_type": "code",
   "source": [
    "ocr_extracted_text = []\n",
    "for path in image_paths:\n",
    "    extract_text_from_image(path)\n",
    "    ocr_extracted_text.append(extract_text_from_image(path))\n",
    "    logger.info(f\"OCR_results: {ocr_extracted_text}\")"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[32m2024-06-10 13:23:53.671\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36m__main__\u001B[0m:\u001B[36m<module>\u001B[0m:\u001B[36m5\u001B[0m - \u001B[1mOCR_results: ['Nutrients: Macronutrients\\n\\n¢ Fats: source of stored energy\\n— Cushion and protect vital organs\\n— Insulate the body in cold weather\\n']\u001B[0m\n",
      "\u001B[32m2024-06-10 13:23:54.376\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36m__main__\u001B[0m:\u001B[36m<module>\u001B[0m:\u001B[36m5\u001B[0m - \u001B[1mOCR_results: ['Nutrients: Macronutrients\\n\\n¢ Fats: source of stored energy\\n— Cushion and protect vital organs\\n— Insulate the body in cold weather\\n', 'Which Describes Active Transport?\\n\\nA. K+ will move from high concentration\\n\\nto low concentration; ATP is used. ‘Retive transport\\nK\\n\\nB. K+ will move from low Low concentration\\nconcentration to high\\nconcentration; ATP is used.\\n\\nC. K+ will move from high concentration\\nto low concentration; ATP is not\\nused.\\n\\nb\\nD. K+ will move from low concentration\\nto high concentration; ATP is not\\nused.\\n']\u001B[0m\n",
      "\u001B[32m2024-06-10 13:23:54.861\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36m__main__\u001B[0m:\u001B[36m<module>\u001B[0m:\u001B[36m5\u001B[0m - \u001B[1mOCR_results: ['Nutrients: Macronutrients\\n\\n¢ Fats: source of stored energy\\n— Cushion and protect vital organs\\n— Insulate the body in cold weather\\n', 'Which Describes Active Transport?\\n\\nA. K+ will move from high concentration\\n\\nto low concentration; ATP is used. ‘Retive transport\\nK\\n\\nB. K+ will move from low Low concentration\\nconcentration to high\\nconcentration; ATP is used.\\n\\nC. K+ will move from high concentration\\nto low concentration; ATP is not\\nused.\\n\\nb\\nD. K+ will move from low concentration\\nto high concentration; ATP is not\\nused.\\n', 'Nutrients: Macronutrients\\n\\n¢ Water\\n— 3 liters lost each day in sweat, urine, feces\\n= 1.5 Lreplaced by food\\n= 1.5 Lneeded from other sources\\n\\n']\u001B[0m\n"
     ]
    }
   ],
   "execution_count": 43
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Generate Embeddings"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-10T11:24:04.469629Z",
     "start_time": "2024-06-10T11:24:04.380895Z"
    }
   },
   "cell_type": "code",
   "source": "dataset_image_embeddings, dataset_text_embeddings = CLIP.generate_dataset_embeddings(clip, image_dataset, ocr_extracted_text)",
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Input Which Describes Active Transport?\n\nA. K+ will move from high concentration\n\nto low concentration; ATP is used. ‘Retive transport\nK\n\nB. K+ will move from low Low concentration\nconcentration to high\nconcentration; ATP is used.\n\nC. K+ will move from high concentration\nto low concentration; ATP is not\nused.\n\nb\nD. K+ will move from low concentration\nto high concentration; ATP is not\nused.\n is too long for context length 77",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[45], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m dataset_image_embeddings, dataset_text_embeddings \u001B[38;5;241m=\u001B[39m \u001B[43mCLIP\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgenerate_dataset_embeddings\u001B[49m\u001B[43m(\u001B[49m\u001B[43mclip\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mimage_dataset\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mocr_extracted_text\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\PycharmProjects\\afm-vlm\\src\\clip\\clip.py:20\u001B[0m, in \u001B[0;36mCLIP.generate_dataset_embeddings\u001B[1;34m(self, image_dataset, texts_descriptions)\u001B[0m\n\u001B[0;32m     15\u001B[0m \u001B[38;5;129m@staticmethod\u001B[39m\n\u001B[0;32m     16\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mgenerate_dataset_embeddings\u001B[39m(\u001B[38;5;28mself\u001B[39m, image_dataset, texts_descriptions) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28mtuple\u001B[39m:\n\u001B[0;32m     17\u001B[0m     dataset_images_preprocessed \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mcat([\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpreprocess(image)\u001B[38;5;241m.\u001B[39munsqueeze(\u001B[38;5;241m0\u001B[39m) \u001B[38;5;28;01mfor\u001B[39;00m image \u001B[38;5;129;01min\u001B[39;00m image_dataset],\n\u001B[0;32m     18\u001B[0m                                             dim\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0\u001B[39m)\u001B[38;5;241m.\u001B[39mto(\n\u001B[0;32m     19\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdevice)\n\u001B[1;32m---> 20\u001B[0m     text_inputs \u001B[38;5;241m=\u001B[39m \u001B[43mclip\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtokenize\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtexts_descriptions\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241m.\u001B[39mto(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdevice)\n\u001B[0;32m     22\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mno_grad():\n\u001B[0;32m     23\u001B[0m         dataset_image_embeddings \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel\u001B[38;5;241m.\u001B[39mencode_image(dataset_images_preprocessed)\n",
      "File \u001B[1;32m~\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\afm-vlm-y3Vw7rqv-py3.10\\lib\\site-packages\\clip\\clip.py:242\u001B[0m, in \u001B[0;36mtokenize\u001B[1;34m(texts, context_length, truncate)\u001B[0m\n\u001B[0;32m    240\u001B[0m             tokens[\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m] \u001B[38;5;241m=\u001B[39m eot_token\n\u001B[0;32m    241\u001B[0m         \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m--> 242\u001B[0m             \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mInput \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mtexts[i]\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m is too long for context length \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mcontext_length\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m    243\u001B[0m     result[i, :\u001B[38;5;28mlen\u001B[39m(tokens)] \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mtensor(tokens)\n\u001B[0;32m    245\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m result\n",
      "\u001B[1;31mRuntimeError\u001B[0m: Input Which Describes Active Transport?\n\nA. K+ will move from high concentration\n\nto low concentration; ATP is used. ‘Retive transport\nK\n\nB. K+ will move from low Low concentration\nconcentration to high\nconcentration; ATP is used.\n\nC. K+ will move from high concentration\nto low concentration; ATP is not\nused.\n\nb\nD. K+ will move from low concentration\nto high concentration; ATP is not\nused.\n is too long for context length 77"
     ]
    }
   ],
   "execution_count": 45
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Load Test Keyframe Image"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-10T11:30:38.326985Z",
     "start_time": "2024-06-10T11:30:37.583176Z"
    }
   },
   "cell_type": "code",
   "source": [
    "test_image_path = os.path.join(base_dir, 'afm-vlm', 'data', 'raw', 'biology_chapter_3_3', 'extracted_keyframes',\n",
    "                                     'biology_chapter_3_3-Scene-099-01.jpg')\n",
    "test_text_descrition = extract_text_from_image(test_image_path)\n",
    "\n",
    "test_image = Image.open(test_image_path)\n",
    "test_image_preprocessed = clip.preprocess(test_image).unsqueeze(0).to(clip.device)\n",
    "\n",
    "# generate embedding for the test keyframe image\n",
    "test_image_embedding = clip.encode_image(test_image_preprocessed)\n",
    "tokenized_text = clip.tokenize_text(test_text_descrition)\n",
    "test_text_embedding = clip.model.encode_text(tokenized_text)\n",
    "\n",
    "logger.info(f\"Test_image_embedding: {test_image_embedding}\")\n",
    "logger.info(f\"Test_text_embedding: {test_text_embedding}\")\n"
   ],
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'CLIP' object has no attribute 'encode_image'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[51], line 9\u001B[0m\n\u001B[0;32m      6\u001B[0m test_image_preprocessed \u001B[38;5;241m=\u001B[39m clip\u001B[38;5;241m.\u001B[39mpreprocess(test_image)\u001B[38;5;241m.\u001B[39munsqueeze(\u001B[38;5;241m0\u001B[39m)\u001B[38;5;241m.\u001B[39mto(clip\u001B[38;5;241m.\u001B[39mdevice)\n\u001B[0;32m      8\u001B[0m \u001B[38;5;66;03m# generate embedding for the test keyframe image\u001B[39;00m\n\u001B[1;32m----> 9\u001B[0m test_image_embedding \u001B[38;5;241m=\u001B[39m \u001B[43mclip\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mencode_image\u001B[49m(test_image_preprocessed)\n\u001B[0;32m     10\u001B[0m tokenized_text \u001B[38;5;241m=\u001B[39m clip\u001B[38;5;241m.\u001B[39mtokenize_text(test_text_descrition)\n\u001B[0;32m     11\u001B[0m test_text_embedding \u001B[38;5;241m=\u001B[39m clip\u001B[38;5;241m.\u001B[39mmodel\u001B[38;5;241m.\u001B[39mencode_text(tokenized_text)\n",
      "\u001B[1;31mAttributeError\u001B[0m: 'CLIP' object has no attribute 'encode_image'"
     ]
    }
   ],
   "execution_count": 51
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Compute Cosine Similarity"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-10T10:20:40.470095Z",
     "start_time": "2024-06-10T10:20:40.448632Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Compute the cosine similarity between the test image embedding and each dataset image embedding\n",
    "cosine_similarity = torch.nn.functional.cosine_similarity(test_image_embedding, dataset_image_embeddings)\n",
    "\n",
    "# Get the index of the image with the highest similarity\n",
    "max_similarity_index = cosine_similarity.argmax().item()\n",
    "logger.info(f\"Max_similarity_index: {max_similarity_index}\")"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[32m2024-06-10 12:20:40.451\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36m__main__\u001B[0m:\u001B[36m<module>\u001B[0m:\u001B[36m6\u001B[0m - \u001B[1mMax_similarity_index: 1\u001B[0m\n"
     ]
    }
   ],
   "execution_count": 65
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "afm-vlm-JEfTnAR4-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
