{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from src.video_preprocessing.download_videos.youtube_download import preprocess_video\n",
    "from src.video_preprocessing.scene_detection.scene_detect import detect_scenes\n",
    "from src.video_preprocessing.scene_detection.ocr import extract_text_from_slide\n",
    "from src.video_preprocessing.download_videos.download_utils import (\n",
    "    transcribe_audio_files,\n",
    "    extract_and_store_audio,\n",
    ")\n",
    "from src.ocr.pytesseract_image_to_text import extract_text_from_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define options and input for downloading a video from youtube\n",
    "\n",
    "# INSERT video name here\n",
    "name = \"biology_chapter_3_3\"\n",
    "# INSERT video URL here\n",
    "url = \"https://youtu.be/DZSEErNZ1d4?si=f6YxKQ9rP6iqgTfk\"\n",
    "# INSERT chunk length in seconds 30s --> 30, no splitting: None\n",
    "chunks = None\n",
    "\n",
    "\n",
    "opts_aud = {\"format\": \"mp3/bestaudio/best\", \"keep-video\": True}\n",
    "opts_vid = {\"format\": \"mp4/bestvideo/best\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Downloads the video creates the relevant datafolders and transcribes the video\n",
    "data_path = preprocess_video(\n",
    "    download=True,\n",
    "    uploaded_vid=\"ignore\",  # path to local file\n",
    "    url=url,\n",
    "    name=name,\n",
    "    aud_opts=opts_aud,\n",
    "    vid_opts=opts_vid,  # Video download settings\n",
    "    audio_file=name + \".mp3\",\n",
    "    input_file=name + \".mp4\",\n",
    "    output=\"output.mp4\",\n",
    "    split_length=chunks,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Â Now that we have downloaded the video we want to perform scene_Detection:\n",
    "detect_scenes(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the audio per detected scene\n",
    "extract_and_store_audio(\n",
    "    os.path.join(data_path, \"scene_snippets\"),\n",
    "    os.path.join(data_path, \"audio_chunks\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transcribe the different snippets snippets:\n",
    "audio_dir = os.path.join(data_path, \"audio_chunks\")\n",
    "transcriptions_dir = os.path.join(data_path, \"transcriptions\")\n",
    "\n",
    "model_type = \"tiny\"  # change to 'large' if you want more accurate results,\n",
    "# change to 'medium.en' or 'large.en' for all english language tasks,\n",
    "# and change to 'small' or 'base' for faster inference\n",
    "lang = \"en\"\n",
    "\n",
    "# Run whisper on all .wav files in audio_dir\n",
    "transcribe_audio_files(audio_dir, transcriptions_dir, model_type=model_type, lang=lang)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import glob\n",
    "\n",
    "directory = \"/Users/magic-rabbit/Documents/AFM/afm-vlm/data/raw/biology_chapter_3_3/extracted_keyframes\"\n",
    "file_pattern = \"*.jpg\"\n",
    "\n",
    "for file_path in glob.glob(f\"{directory}/{file_pattern}\"):\n",
    "    text = extract_text_from_image(file_path)\n",
    "    print(f\"Text for the following path:{file_path} is: {text}\")\n",
    "\n",
    "\n",
    "# extract_text_from_slide(extracted_keyframe_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llm.ollama_implementation.ollama_experiment import generate_response\n",
    "\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# Read the .csv file\n",
    "df = pd.read_csv(\n",
    "    \"/Users/magic-rabbit/Documents/AFM/afm-vlm/data/raw/biology_chapter_3_3/transcriptions/biology_chapter_3_3-Scene-055.csv\"\n",
    ")\n",
    "\n",
    "# Combine all content from the \"text\" column into one string\n",
    "transcription = \" \".join(df[\"text\"].astype(str))\n",
    "text = extract_text_from_image(\n",
    "    \"/Users/magic-rabbit/Documents/AFM/afm-vlm/data/raw/biology_chapter_3_3/extracted_keyframes/biology_chapter_3_3-Scene-055-01.jpg\"\n",
    ")\n",
    "start_time = time.time()\n",
    "response = generate_response(slide_content=text, transcription=transcription)\n",
    "# Calculate the elapsed time\n",
    "elapsed_time = time.time() - start_time\n",
    "\n",
    "# Print the elapsed time\n",
    "print(\"Elapsed Time:\", elapsed_time, \"seconds\")\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import CLIPProcessor, CLIPModel, CLIPTokenizer\n",
    "import torch\n",
    "\n",
    "\n",
    "def get_model_info(model_ID, device):\n",
    "    # Save the model to device\n",
    "    model = CLIPModel.from_pretrained(model_ID).to(device)\n",
    "    # Get the processor\n",
    "    processor = CLIPProcessor.from_pretrained(model_ID)\n",
    "    # Get the tokenizer\n",
    "    tokenizer = CLIPTokenizer.from_pretrained(model_ID)\n",
    "    # Return model, processor & tokenizer\n",
    "    return model, processor, tokenizer\n",
    "\n",
    "\n",
    "# Set the device\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "# Define the model ID\n",
    "model_ID = \"openai/clip-vit-base-patch32\"\n",
    "# Get model, processor & tokenizer\n",
    "model, processor, tokenizer = get_model_info(model_ID, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "\n",
    "def get_image(image_path):\n",
    "    image = Image.open(image_path)\n",
    "    # Convert the image to RGB\n",
    "    rgb_image = image.convert(\"RGB\")\n",
    "    return rgb_image\n",
    "\n",
    "\n",
    "def get_single_image_embedding(text, my_image, processor, model, device):\n",
    "    image = processor(text=text, images=my_image, return_tensors=\"pt\")[\n",
    "        \"pixel_values\"\n",
    "    ].to(device)\n",
    "    embedding = model.get_image_features(image)\n",
    "    # convert the embeddings to numpy array\n",
    "    return embedding.cpu().detach().numpy()\n",
    "\n",
    "\n",
    "one_image = get_image(\n",
    "    image_path=\"/Users/magic-rabbit/Documents/AFM/afm-vlm/data/raw/biology_chapter_3_3/extracted_keyframes/biology_chapter_3_3-Scene-055-01.jpg\"\n",
    ")\n",
    "\n",
    "\n",
    "one_vector = get_single_image_embedding(\n",
    "    response, one_image, processor, model, device\n",
    ")  # Simple test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(one_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "afm-vlm-JEfTnAR4-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
