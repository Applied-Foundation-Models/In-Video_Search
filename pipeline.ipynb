{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-30T15:49:06.403550Z",
     "start_time": "2024-06-30T15:49:06.028105Z"
    }
   },
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from src.video_preprocessing.download_videos.youtube_download import preprocess_video\n",
    "from src.video_preprocessing.scene_detection.scene_detect import detect_scenes\n",
    "from src.video_preprocessing.download_videos.download_utils import (\n",
    "    transcribe_audio_files,\n",
    "    extract_and_store_audio,\n",
    ")\n",
    "from src.ocr.pytesseract_image_to_text import extract_text_from_image\n",
    "\n",
    "from src.llm.ollama_implementation.ollama_experiment import (\n",
    "    prompt_llm_summary,\n",
    "    prompt_llm_extensive_summary,\n",
    ")\n",
    "from src.video_preprocessing.download_videos.download_utils import (\n",
    "    transcription_to_text,\n",
    "    create_metadata,\n",
    ")\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "from loguru import logger\n",
    "import pickle\n",
    "\n",
    "from src.clip.clip_model import CLIPEmbeddingsModel\n",
    "\n",
    "import tqdm\n",
    "from pathlib import Path\n",
    "import os"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download Pipeline\n",
    "Download a video from a specific URL on YouTube, then run:\n",
    "- Scene detection\n",
    "- Keyframe detection\n",
    "\n",
    "The resulting data will be stored under `/data/raw/<NAME>`"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-30T15:49:07.746077Z",
     "start_time": "2024-06-30T15:49:07.493083Z"
    }
   },
   "source": [
    "# Define options and input for downloading a video from youtube\n",
    "\n",
    "# INSERT video name here\n",
    "name = \"biology_chapter_3_3\"\n",
    "# INSERT video URL here\n",
    "url = \"https://youtu.be/DZSEErNZ1d4?si=f6YxKQ9rP6iqgTfk\"\n",
    "# INSERT chunk length in seconds 30s --> 30, no splitting: None\n",
    "chunks = None\n",
    "\n",
    "opts_aud = {\"format\": \"mp3/bestaudio/best\", \"keep-video\": True}\n",
    "opts_vid = {\"format\": \"mp4/bestvideo/best\"}"
   ],
   "outputs": [],
   "execution_count": 21
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-30T15:49:10.869970Z",
     "start_time": "2024-06-30T15:49:08.303046Z"
    }
   },
   "source": [
    "# Downloads the video creates the relevant datafolders and transcribes the video\n",
    "data_path = preprocess_video(\n",
    "    download=True,\n",
    "    uploaded_vid=\"ignore\",  # path to local file\n",
    "    url=url,\n",
    "    name=name,\n",
    "    aud_opts=opts_aud,\n",
    "    vid_opts=opts_vid,  # Video download settings\n",
    "    audio_file=name + \".mp3\",\n",
    "    input_file=name + \".mp4\",\n",
    "    output=\"output.mp4\",\n",
    "    split_length=chunks,\n",
    ")"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[32m2024-06-30 17:49:08.521\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.video_preprocessing.download_videos.youtube_download\u001B[0m:\u001B[36mpreprocess_video\u001B[0m:\u001B[36m49\u001B[0m - \u001B[1mStarting AutoCaptioning...\u001B[0m\n",
      "\u001B[32m2024-06-30 17:49:08.522\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.video_preprocessing.download_videos.youtube_download\u001B[0m:\u001B[36mpreprocess_video\u001B[0m:\u001B[36m50\u001B[0m - \u001B[1mResults will be stored in data/raw/biology_chapter_3_3\u001B[0m\n",
      "\u001B[32m2024-06-30 17:49:08.522\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.video_preprocessing.download_videos.youtube_download\u001B[0m:\u001B[36mpreprocess_video\u001B[0m:\u001B[36m58\u001B[0m - \u001B[1mCreated chunks folders\u001B[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[youtube] Extracting URL: https://youtu.be/DZSEErNZ1d4?si=f6YxKQ9rP6iqgTfk\n",
      "[youtube] DZSEErNZ1d4: Downloading webpage\n",
      "[youtube] DZSEErNZ1d4: Downloading ios player API JSON\n",
      "[youtube] DZSEErNZ1d4: Downloading m3u8 information\n",
      "[info] DZSEErNZ1d4: Downloading 1 format(s): 18\n",
      "[download] C:\\Users\\baatout\\PycharmProjects\\afm-vlm\\data\\raw\\biology_chapter_3_3\\biology_chapter_3_3.mp4 has already been downloaded\n",
      "[download] 100% of   85.73MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[32m2024-06-30 17:49:10.864\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.video_preprocessing.download_videos.youtube_download\u001B[0m:\u001B[36mpreprocess_video\u001B[0m:\u001B[36m91\u001B[0m - \u001B[1mVideo is not splitted:\u001B[0m\n",
      "\u001B[32m2024-06-30 17:49:10.865\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.video_preprocessing.download_videos.youtube_download\u001B[0m:\u001B[36mpreprocess_video\u001B[0m:\u001B[36m95\u001B[0m - \u001B[1mVideo downloaded successfully!\u001B[0m\n"
     ]
    }
   ],
   "execution_count": 22
  },
  {
   "cell_type": "code",
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2024-06-25T18:21:09.532343Z"
    }
   },
   "source": [
    "#  Now that we have downloaded the video we want to perform scene_Detection:\n",
    "detect_scenes(data_path)"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[32m2024-06-25 20:21:09.819\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.video_preprocessing.scene_detection.scene_detect\u001B[0m:\u001B[36mdetect_scenes\u001B[0m:\u001B[36m29\u001B[0m - \u001B[1mFound file\u001B[0m\n",
      "\u001B[32m2024-06-25 20:21:09.835\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.video_preprocessing.scene_detection.scene_detect\u001B[0m:\u001B[36mdetect_scenes\u001B[0m:\u001B[36m33\u001B[0m - \u001B[1mName:biology_chapter_3_3.mp4,dirname:C:\\Users\\baatout\\PycharmProjects\\afm-vlm\\data/raw\\biology_chapter_3_3\\biology_chapter_3_3.mp4\u001B[0m\n",
      "\u001B[32m2024-06-25 20:21:09.837\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.video_preprocessing.scene_detection.scene_detect\u001B[0m:\u001B[36mdetect_scenes\u001B[0m:\u001B[36m35\u001B[0m - \u001B[1mRunning scene_detection:\u001B[0m\n"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-17T08:43:22.576364Z",
     "start_time": "2024-06-17T08:41:53.940840Z"
    }
   },
   "outputs": [],
   "source": [
    "# Extract the audio per detected scene\n",
    "extract_and_store_audio(\n",
    "    os.path.join(data_path, \"scene_snippets\"),\n",
    "    os.path.join(data_path, \"audio_chunks\"),\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Audio Transcription using Whisper\n",
    "\n",
    "For Faster Inference Please Use Tiny!"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-25T22:02:01.807595Z",
     "start_time": "2024-06-25T21:57:07.223110Z"
    }
   },
   "source": [
    "# Transcribe the different snippets:\n",
    "audio_dir = os.path.join(data_path, \"audio_chunks\")\n",
    "transcriptions_dir = os.path.join(data_path, \"transcriptions\")\n",
    "\n",
    "model_type = \"tiny\"  # change to 'large' if you want more accurate results,\n",
    "# change to 'medium.en' or 'large.en' for all english language tasks,\n",
    "# and change to 'small' or 'base' for faster inference\n",
    "lang = \"en\"\n",
    "\n",
    "# Run whisper on all .wav files in audio_dir\n",
    "transcribe_audio_files(audio_dir, transcriptions_dir, model_type=model_type, lang=lang)"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[32m2024-06-25 23:57:07.443\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.video_preprocessing.download_videos.download_utils\u001B[0m:\u001B[36mtranscribe_audio_files\u001B[0m:\u001B[36m361\u001B[0m - \u001B[1mStarting pooling:\u001B[0m\n",
      "100%|██████████| 141/141 [04:53<00:00,  2.08s/it]\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the CLIP Model"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-30T15:49:18.722129Z",
     "start_time": "2024-06-30T15:49:14.438930Z"
    }
   },
   "source": [
    "# create instance\n",
    "clip_model = CLIPEmbeddingsModel()"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 800x2000 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 23
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-30T15:49:19.010380Z",
     "start_time": "2024-06-30T15:49:18.723141Z"
    }
   },
   "source": [
    "# get current directory\n",
    "# Get the path of the current notebook\n",
    "notebook_path = Path().resolve()\n",
    "image_path = os.path.join(\n",
    "    notebook_path, \"data\", \"raw\", \"biology_chapter_3_3_treshhold_5\", \"extracted_keyframes\"\n",
    ")"
   ],
   "outputs": [],
   "execution_count": 24
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Starting the Analysis of the Information Contained in the Video\n",
    "\n",
    "### Inputs\n",
    "\n",
    "* **Transcriptions**: [insert description or link to transcription]\n",
    "* **Extraction from Slides using OCR**: [insert description or link to extracted content]\n",
    "* **Textual Interpretation of Visual Information using LLAVA**: [insert description or link to \n",
    "interpreted information]"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-30T14:44:02.450073Z",
     "start_time": "2024-06-30T14:44:01.516245Z"
    }
   },
   "source": [
    "# Transform transcription file\n",
    "keyframes = {}\n",
    "ocr_extracted_text = []\n",
    "\n",
    "for filename in tqdm.tqdm(os.listdir(image_path)):\n",
    "    # Check if the file ends with the specified extension\n",
    "    if filename.endswith(\".jpg\"):\n",
    "        filepath = os.path.join(image_path, filename)\n",
    "        keyframe_num = int(filename.split(\"-\")[2])\n",
    "\n",
    "        # Extract text using OCR:\n",
    "        ocr_text = extract_text_from_image(filepath)\n",
    "\n",
    "        # logger.info(f\"OCR_results: {ocr_text}\")\n",
    "\n",
    "        transcription_file_path = os.path.join(\n",
    "            os.path.dirname(image_path),\n",
    "            \"transcriptions\",\n",
    "            filename.replace(\"-01.jpg\", \".csv\"),\n",
    "        )\n",
    "\n",
    "        transcription, timestamps = transcription_to_text(transcription_file_path)\n",
    "        logger.info(f\"Transcription_text: {transcription}\")\n",
    "\n",
    "        # Extract textual understanding of Visual features using LLAVA:\n",
    "\n",
    "        #llava_results = generate_caption_using_llava(filepath)\n",
    "        llava_results = \"llava_results\"\n",
    "        logger.info(f\"LLava_results: {llava_results}\")\n",
    "\n",
    "        clip_llm_summary = prompt_llm_summary(\n",
    "            slide_content=ocr_text,\n",
    "            transcription=transcription,\n",
    "            llava_output=llava_results,\n",
    "        )\n",
    "\n",
    "        extensive_summary = prompt_llm_extensive_summary(\n",
    "            slide_content=ocr_extracted_text,\n",
    "            transcription=transcription,\n",
    "            llava_output=llava_results,\n",
    "        )\n",
    "\n",
    "        # Alternative that goes faster.\n",
    "        # ocr_text = \"ocr_text\"\n",
    "        # llava_results = \"llava_results\"\n",
    "        # clip_llm_summary = \"clip_llm_summary\"\n",
    "        # extensive_summary = \"extensive_summary\"\n",
    "\n",
    "        # generate embeddings\n",
    "        opened_image = Image.open(filepath)\n",
    "\n",
    "        embeddings = clip_model.generate_image_embeddings(\n",
    "            clip_llm_summary, opened_image\n",
    "        )\n",
    "        clip_text_embedding = embeddings[\"text_embeds\"]\n",
    "        clip_image_embedding = embeddings[\"image_embeds\"]\n",
    "\n",
    "        keyframe, keyframe_metadata = create_metadata(\n",
    "            keyframe_num,\n",
    "            filepath,\n",
    "            timestamps,\n",
    "            transcription,\n",
    "            ocr_extracted_text,\n",
    "            llava_results,\n",
    "            clip_llm_summary,\n",
    "            extensive_summary,\n",
    "            clip_text_embedding,\n",
    "            clip_image_embedding,\n",
    "        )\n",
    "        keyframes[keyframe] = keyframe_metadata\n",
    "        # print(keyframes)\n",
    "\n",
    "    # Save keyframes dictionary as Pickle\n",
    "\n",
    "# Save with pickle\n",
    "with open(\"data.pickle\", \"wb\") as file:\n",
    "    pickle.dump(keyframes, file)"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/87 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "transcription_to_text() missing 2 required positional arguments: 'transcription_file_path' and 'timestamp_file_path'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[8], line 22\u001B[0m\n\u001B[0;32m     14\u001B[0m \u001B[38;5;66;03m# logger.info(f\"OCR_results: {ocr_text}\")\u001B[39;00m\n\u001B[0;32m     16\u001B[0m transcription_file_path \u001B[38;5;241m=\u001B[39m os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39mjoin(\n\u001B[0;32m     17\u001B[0m     os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39mdirname(image_path),\n\u001B[0;32m     18\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtranscriptions\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m     19\u001B[0m     filename\u001B[38;5;241m.\u001B[39mreplace(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m-01.jpg\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m.csv\u001B[39m\u001B[38;5;124m\"\u001B[39m),\n\u001B[0;32m     20\u001B[0m )\n\u001B[1;32m---> 22\u001B[0m transcription, timestamps \u001B[38;5;241m=\u001B[39m \u001B[43mtranscription_to_text\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtranscription_file_path\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     23\u001B[0m logger\u001B[38;5;241m.\u001B[39minfo(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mTranscription_text: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mtranscription\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m     25\u001B[0m \u001B[38;5;66;03m# Extract textual understanding of Visual features using LLAVA:\u001B[39;00m\n\u001B[0;32m     26\u001B[0m \n\u001B[0;32m     27\u001B[0m \u001B[38;5;66;03m#llava_results = generate_caption_using_llava(filepath)\u001B[39;00m\n",
      "\u001B[1;31mTypeError\u001B[0m: transcription_to_text() missing 2 required positional arguments: 'transcription_file_path' and 'timestamp_file_path'"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-30T14:44:09.597918Z",
     "start_time": "2024-06-30T14:44:09.369306Z"
    }
   },
   "source": [
    "# Assuming keyframes is your dictionary\n",
    "keyframes_count = len(keyframes)\n",
    "print(\"Number of keyframes:\", keyframes_count)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of keyframes: 0\n"
     ]
    }
   ],
   "execution_count": 9
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "afm-vlm-JEfTnAR4-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
